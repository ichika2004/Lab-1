{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:吳宗霖\n",
    "\n",
    "Student ID:B114020045\n",
    "\n",
    "GitHub ID:ichika2004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Phase Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) that considered as **phase 1 (from exercise 1 to exercise 15)**. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** up **until phase 1**. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    -  Use [the new dataset](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/newdataset/Reddit-stock-sentiment.csv). The dataset contains a 16 columns including 'text' and 'label', with the sentiment labels being: 1.0 is positive, 0.0 is neutral and -1.0 is negative. You can simplify the dataset and use only the columns that you think are necessary. \n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "    - Use this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 10% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    \n",
    "\n",
    "\n",
    "4. Fourth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (September 28th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can keep the answer for phase 1 for easier running and update the phase 2 on the same page.**\n",
    "\n",
    "1. First: Continue doing the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) for **phase 2, starting from Finding frequent patterns**. Use the same master(.ipynb) file. Answer from phase 1 will not be considered at this stage. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: Continue from first phase and do the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** for phase 2, starting from Finding frequent pattern. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    - Continue using this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output. Use the same new dataset as in phase 1.\n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 20% of your grade.__\n",
    "    - Use this file to answer.\n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency).  Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences and when using augmentation with feature pattern.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 19th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt') # download the NLTK datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import math\n",
    "import PAMI\n",
    "import umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./newdataset/Reddit-stock-sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   type          847 non-null    object \n",
      " 1   datetime      847 non-null    object \n",
      " 2   post_id       847 non-null    object \n",
      " 3   subreddit     847 non-null    object \n",
      " 4   title         847 non-null    object \n",
      " 5   author        847 non-null    object \n",
      " 6   url           847 non-null    object \n",
      " 7   upvotes       847 non-null    int64  \n",
      " 8   downvotes     64 non-null     float64\n",
      " 9   upvote_ratio  64 non-null     float64\n",
      " 10  text          847 non-null    object \n",
      " 11  subjectivity  847 non-null    float64\n",
      " 12  polarity      847 non-null    float64\n",
      " 13  sentiment     847 non-null    float64\n",
      " 14  entities      847 non-null    object \n",
      " 15  label         847 non-null    float64\n",
      "dtypes: float64(6), int64(1), object(9)\n",
      "memory usage: 106.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 17:29:56</td>\n",
       "      <td>mmli62w</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retardation is on the menu boys! WSB is so back</td>\n",
       "      <td>StickyTip420</td>\n",
       "      <td>https://i.redd.it/0yq2ftren8ue1.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calls on retards</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>calls on retards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-12 1:12:19</td>\n",
       "      <td>mmnu7v9</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Retail giant TARGET has now declined for 10 co...</td>\n",
       "      <td>Comfortable-Dog-8437</td>\n",
       "      <td>https://i.redd.it/7tl6puv9waue1.jpeg</td>\n",
       "      <td>-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stunt as in like why did they even make a big ...</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Stunt', 'company', 'deal', 'place']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stunt as in like why did they even make a big ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-10 15:09:41</td>\n",
       "      <td>mmeevio</td>\n",
       "      <td>StockMarket</td>\n",
       "      <td>How do you feel about a sitting president maki...</td>\n",
       "      <td>Btankersly66</td>\n",
       "      <td>https://apnews.com/article/trump-truth-social-...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seeing lots of red in the ticker.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['ticker']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>seeing lots of red in the ticker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>2023-08-30 17:12:55</td>\n",
       "      <td>165kllm</td>\n",
       "      <td>stockstobuytoday</td>\n",
       "      <td>Who knows more? $VMAR</td>\n",
       "      <td>emiljenfn</td>\n",
       "      <td>https://www.reddit.com/r/stockstobuytoday/comm...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Vision Marine Technologies Inc. is rewriting t...</td>\n",
       "      <td>0.646970</td>\n",
       "      <td>0.216383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['watercraft', 'skill', 'power', ']', 'feat', ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>vision marine technologies inc is rewriting th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-04-11 14:48:05</td>\n",
       "      <td>mmkl6bw</td>\n",
       "      <td>StockMarket</td>\n",
       "      <td>The Trump administration is begging Xi Jinping...</td>\n",
       "      <td>Just-Big6411</td>\n",
       "      <td>https://edition.cnn.com/2025/04/10/politics/tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He didn’t say thank you.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>he didnt say thank you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type             datetime  post_id         subreddit  \\\n",
       "0  comment  2025-04-11 17:29:56  mmli62w    wallstreetbets   \n",
       "1  comment   2025-04-12 1:12:19  mmnu7v9    wallstreetbets   \n",
       "2  comment  2025-04-10 15:09:41  mmeevio       StockMarket   \n",
       "3     post  2023-08-30 17:12:55  165kllm  stockstobuytoday   \n",
       "4  comment  2025-04-11 14:48:05  mmkl6bw       StockMarket   \n",
       "\n",
       "                                               title                author  \\\n",
       "0    Retardation is on the menu boys! WSB is so back          StickyTip420   \n",
       "1  Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
       "2  How do you feel about a sitting president maki...          Btankersly66   \n",
       "3                              Who knows more? $VMAR             emiljenfn   \n",
       "4  The Trump administration is begging Xi Jinping...          Just-Big6411   \n",
       "\n",
       "                                                 url  upvotes  downvotes  \\\n",
       "0               https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
       "1               https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
       "2  https://apnews.com/article/trump-truth-social-...        1        NaN   \n",
       "3  https://www.reddit.com/r/stockstobuytoday/comm...       30        0.0   \n",
       "4  https://edition.cnn.com/2025/04/10/politics/tr...        1        NaN   \n",
       "\n",
       "   upvote_ratio                                               text  \\\n",
       "0           NaN                                   Calls on retards   \n",
       "1           NaN  Stunt as in like why did they even make a big ...   \n",
       "2           NaN                  Seeing lots of red in the ticker.   \n",
       "3          0.98  Vision Marine Technologies Inc. is rewriting t...   \n",
       "4           NaN                           He didn’t say thank you.   \n",
       "\n",
       "   subjectivity  polarity  sentiment  \\\n",
       "0      1.000000 -0.900000       -1.0   \n",
       "1      0.177778  0.083333        1.0   \n",
       "2      0.000000  0.000000        0.0   \n",
       "3      0.646970  0.216383        1.0   \n",
       "4      0.000000  0.000000        0.0   \n",
       "\n",
       "                                            entities  label  \\\n",
       "0                                                 []   -1.0   \n",
       "1              ['Stunt', 'company', 'deal', 'place']    0.0   \n",
       "2                                         ['ticker']    0.0   \n",
       "3  ['watercraft', 'skill', 'power', ']', 'feat', ...    1.0   \n",
       "4                                                 []   -1.0   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                                   calls on retards  \n",
       "1  stunt as in like why did they even make a big ...  \n",
       "2                   seeing lots of red in the ticker  \n",
       "3  vision marine technologies inc is rewriting th...  \n",
       "4                             he didnt say thank you  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 文本清理前後對比 (前 5 筆) ---\n",
      "                                                text  \\\n",
      "0                                   Calls on retards   \n",
      "1  Stunt as in like why did they even make a big ...   \n",
      "2                  Seeing lots of red in the ticker.   \n",
      "3  Vision Marine Technologies Inc. is rewriting t...   \n",
      "4                           He didn’t say thank you.   \n",
      "\n",
      "                                      processed_text  \n",
      "0                                      calls retards  \n",
      "1  stunt even make big deal starting first place ...  \n",
      "2                             seeing lots red ticker  \n",
      "3  vision marine technologies inc rewriting water...  \n",
      "4                                    didnt say thank  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['cleaned_text'] = df['text'].str.lower()\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(\n",
    "    lambda x: re.sub(r'[^a-z0-9\\s]', '', x)\n",
    ")\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(\n",
    "    lambda x: re.sub(r'\\d+', '', x)\n",
    ")\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "financial_stopwords = {'stock', 'buy', 'sell', 'like', 'get', 'would', 'one', 'go'} \n",
    "\n",
    "all_stopwords = english_stopwords.union(financial_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in all_stopwords and len(word) > 1]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['processed_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\n--- 文本清理前後對比 (前 5 筆) ---\")\n",
    "print(df[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# 假設 df 已經載入並完成小寫化、移除標點等初步清理\n",
    "# -----------------------------------------------------------\n",
    "# 執行文本清理的關鍵步驟：移除停用詞\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "financial_stopwords = {'stock', 'buy', 'sell', 'like', 'get', 'would', 'one', 'go', 'know'}\n",
    "all_stopwords = english_stopwords.union(financial_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # 確保文本不是浮點數或 None\n",
    "    if pd.isna(text): return \"\"\n",
    "    \n",
    "    # 重新運行清理步驟以確保一致性（如果前面的步驟沒問題，這會是重複的）\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in all_stopwords and len(word) > 1]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 建立 TDM\n",
    "data_to_vectorize = df['processed_text'].dropna()\n",
    "count_vect = CountVectorizer(max_features=5000)\n",
    "\n",
    "X_tdm = count_vect.fit_transform(data_to_vectorize)\n",
    "terms = count_vect.get_feature_names_out()\n",
    "\n",
    "tdm_df = pd.DataFrame(X_tdm.toarray(), columns=terms, index=data_to_vectorize.index)\n",
    "\n",
    "print(f\"TDM 形狀: {tdm_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 將 TDM 轉換為 TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_tdm)\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=terms, index=data_to_vectorize.index)\n",
    "\n",
    "print(f\"TF-IDF 矩陣形狀: {tfidf_df.shape}\")\n",
    "print(\"TF-IDF 轉換完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.frequentPattern.basic import FPGrowth\n",
    "import time\n",
    "\n",
    "MIN_SUP = 20\n",
    "\n",
    "print(f\"\\n--- 執行 FPGrowth (Min Sup = {MIN_SUP}) ---\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    fp_obj = FPGrowth.FPGrowth(iFile=reddit_transactions, minSup=MIN_SUP)\n",
    "    fp_patterns = fp_obj.mine()\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    if fp_patterns is None:\n",
    "        fp_patterns = {}\n",
    "        print(\"!!! 警告：FPGrowth 返回 None。已將結果替換為空集。\")\n",
    "    \n",
    "    pattern_count = len(fp_patterns)\n",
    "    \n",
    "    print(f\"運行時間: {runtime:.4f} 秒\")\n",
    "    print(f\"找到的總頻繁模式數量: {pattern_count}\")\n",
    "    \n",
    "    if pattern_count > 0:\n",
    "        print(\"部分 FPGrowth 模式 (模式: 支持度):\", list(fp_patterns.items())[:3])\n",
    "    else:\n",
    "        print(\"未找到任何模式。請嘗試降低 Min Sup。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! FPGrowth 執行失敗 !!!\")\n",
    "    print(f\"錯誤訊息: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "y = df.loc[tfidf_df.index, 'label'].astype(int) \n",
    "X = tfidf_df # 使用 TF-IDF 矩陣作為特徵\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y \n",
    ")\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"             Reddit 情感分類結果 (基於 TF-IDF)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"模型準確度 (Accuracy): {accuracy:.4f}\")\n",
    "print(\"\\n分類報告 (Classification Report):\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_words = tdm_df.sum().sort_values(ascending=False).head(50)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=top_words.index, y=top_words.values, palette=\"viridis\")\n",
    "\n",
    "plt.title('Top 50 Most Frequent Words in Reddit Stock Sentiment Data', fontsize=16)\n",
    "plt.xlabel('Word', fontsize=14)\n",
    "plt.ylabel('Total Document Count', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='label', data=df, palette=\"coolwarm\")\n",
    "\n",
    "plt.title('Distribution of Sentiment Labels', fontsize=16)\n",
    "plt.xlabel('Sentiment Label (Y Variable)', fontsize=14)\n",
    "plt.ylabel('Number of Documents', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Env: DM2025 Lab",
   "language": "python",
   "name": "ai_cup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
